{
  "id": 28361,
  "name": "Streamstraight",
  "slug": "streamstraight",
  "former_names": [
    "Codex",
    "Lightski"
  ],
  "small_logo_thumb_url": "https://bookface-images.s3.amazonaws.com/small_logos/2d21bd8c7920701952e3692272a988b9e7b8a0e5.png",
  "website": "https://www.streamstraight.com",
  "all_locations": "San Francisco, CA, USA",
  "long_description": "Streamstraight ensures your users never see an interrupted LLM response. We solve the problems around resumable streams and reconnecting during an in-progress AI response, so you can ensure your long-running AI agents responses are never interrupted on the frontend. \r\n\r\nIf you…\r\n\r\n* show an incomplete response when users return to an AI chat that's still in-progress\r\n* have users who request multiple long-running AI responses, simultaneously\r\n* lose data when a client reloads the page during a stream\r\n* deal with flaky connections due to mobile clients\r\n* want to run LLM inference from an async job\r\n* want to stream the same LLM response to multiple browser tabs/clients\r\n\r\n…message us! We can fix these issues and ensure your frontend stays high quality during long-running streams. ",
  "one_liner": "The best way to stream LLM thinking traces to your frontend",
  "team_size": 1,
  "industry": "B2B",
  "subindustry": "B2B -> Engineering, Product and Design",
  "launched_at": 1678895856,
  "tags": [
    "Developer Tools",
    "Generative AI",
    "SaaS",
    "B2B"
  ],
  "tags_highlighted": [],
  "top_company": false,
  "isHiring": false,
  "nonprofit": false,
  "batch": "Winter 2023",
  "status": "Active",
  "industries": [
    "B2B",
    "Engineering, Product and Design"
  ],
  "regions": [
    "United States of America",
    "America / Canada"
  ],
  "stage": "Early",
  "app_video_public": false,
  "demo_day_video_public": false,
  "app_answers": null,
  "question_answers": false,
  "url": "https://www.ycombinator.com/companies/streamstraight",
  "api": "https://yc-oss.github.io/api/batches/winter-2023/streamstraight.json"
}
