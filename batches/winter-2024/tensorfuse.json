{
  "id": 29378,
  "name": "Tensorfuse",
  "slug": "tensorfuse",
  "former_names": [
    "10K",
    "TensorFuse"
  ],
  "small_logo_thumb_url": "https://bookface-images.s3.amazonaws.com/small_logos/2bef07502fc1183c53ca01386fbdf17f2db71ba2.png",
  "website": "https://www.tensorfuse.io/",
  "all_locations": "",
  "long_description": "Tensorfuse helps you run fast, scalable AI inference in your own AWS account. Run any model, use any inference server (vLLM, TensorRT, Dynamo) and get ready to scale your AI inference to 1000s of users - all set up in under 60 mins\r\n\r\nJust bring:\r\n1. Your code and env as Dockerfile\r\n2. Your AWS account with GPU capacity\r\n\r\nWe handle the restâ€”deploying, managing, and autoscaling your GPU containers on production-grade infrastructure. ",
  "one_liner": "Run serverless GPUs on your own cloud",
  "team_size": 2,
  "industry": "B2B",
  "subindustry": "B2B -> Engineering, Product and Design",
  "launched_at": 1708013970,
  "tags": [],
  "tags_highlighted": [],
  "top_company": false,
  "isHiring": false,
  "nonprofit": false,
  "batch": "Winter 2024",
  "status": "Active",
  "industries": [
    "B2B",
    "Engineering, Product and Design"
  ],
  "regions": [
    "Unspecified"
  ],
  "stage": "Early",
  "app_video_public": false,
  "demo_day_video_public": false,
  "app_answers": null,
  "question_answers": false,
  "url": "https://www.ycombinator.com/companies/tensorfuse",
  "api": "https://yc-oss.github.io/api/batches/winter-2024/tensorfuse.json"
}
